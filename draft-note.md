function describe the world

符号主义的思路就是通过人类的观察总结发现事物背后的一个规律，通过这个规律，人们可以给规律、输入，然后得到输出，比如说勾股定理

但是随着人类研究的课题越来越复杂，符号主义走到尽头了，很多事物的规律已经复杂到人类没办法去通过一个明确的函数来总结，比如说翻译问题，比如说天气预测

但是对于这类问题，人类可以轻松拿到输入数据和输出数据，只是不知道中间的规律是怎么描述的，于是就产生了另外一种思路，就是拿到输入拿到输出，然后根据输入和输出的对应关系来猜测中间的规律是什么先猜一个结果，然后再计算猜测的这个规律得到的结果和期望值的偏差，通过把这个偏差最小化来得到这个规律，近似说这个规律就是我们发现的规律了，这个思路不同于符号主义，这是连接主义的思想

以最简单的一元一次方程为例，y等于kx加b，这一条直线我们在中学的时候就学过了，可以拿来用最小二乘法来做回归，这是我们最早接触连接主义思想的一个例子。现实世界中很多规律都是非线性的，因此我们需要搭建一个函数来引入非线性性，让我们的函数能够更好地拟合现实生活中的一些规律。

我们引入了非线性函数，在神经网络中我们就叫做激活函数，最常用的就两个，一个是sigmod，一个是relu。

回到那个一元一次函数的例子，我们现实生活中输入的数据不一定是只有一个自变量的，可能会有多个输入，因此对应就有多个K作为权重，在这里我们可以把K换成W，更好地表示权重这个概念。然后有时候我们可能一层激活函数做线性引入的非线性信息还不够，我们可以再嵌套多几个这样的激活函数，这样子我们就可以做出非常非常复杂的一个神经网络的函数结构了

然后我们增加树的方式就会在神经元的结构上纵向添加神经元单元，我们得到的那个替换函数的层数对应的就是输入层和输出层之间的隐藏层的层数

然后由多个输入经过隐藏层之后，最后得到一个输出层，这样的网络结构，我们整个就叫做神经网络

输入层经过线性变换进入隐藏层，通过隐藏层，经过激活函数到达输出层的这个过程，自变量从输入到激活函数再到输出的传播路径，我们称之为前向传播，其实就是通过自变量一层一层地把结果不断地算出来而已

然后求解一个神经网络的过程就是根据已知的X1到Xn的所有输入和Y的所有输出，来估计我们中间的W1到WN、B1到BN这些变量的值是多少。

那么我们如何去计算这些W和这些B呢，一般来讲，我们会先随机初始化一些W和B的值，然后假设是一元一次方程，我们可以得到一条直线。根据这条直线上落在对应的x1到x2上的这些点，我们可以得到y1到yn的预测值。

通过计算预测值和真实的y1到yn的真实值的差值的绝对值，我们就能得到预测的误差。为了计算方便，我们不会使用绝对值，而是使用差值的平方，使得所有的误差都为正。然后我们对所有误差做求和，这样就得到了估计的误差。

除以样本的个数，比如我们抽了n个点，就除以n，这样我们得到了均方误差。除以n可以理解为平均掉样本点的个数对误差造成的影响，不然对于同样一个规律，我们采10个样本点和100个样本点时，误差会差十倍，但我们的模型其实只有一个。那最后算出来的这两个神经网络模型可能就差得很远。这样的比较就不太公平了，所以我们会除以n算平均分值

然后一般来讲，我们会拿这个均方误差作为我们要降低到最小的那个损失函数

所以我们的损失函数实际上就是预测值和真实值之间的误差，然后我们训练一个神经网络模型的目标就是求解这个让损失函数达到最小值的w和b

然后学过微积分的话呢，我们就知道怎么样求这一个函数的极小值呢，我们会选择一些使其导数等于零的点作为极值点。这些极值点可能就是极小值或者极大值，具体是极小值还是极大值呢，我们可以通过判断这个极值点左右两边的符号来判断。
是极小值还是极大值，另外我们通过梯度下降算法的话，我们找到的就肯定是极小值，因为我们是按照梯度下降的那个趋势，是去找到的这个极值点为零的点，那么我们每一次计算的上一跳它的导数肯定是比下一跳的要大的，那最终我们得到的点那肯定是极小值点啊就像往一个坑坑洼洼的水泥地里面丢一个乒乓球一样，因为重力的因素，这个乒乓球肯定会在这个坑坑洼洼的地面上不断的滑行，直到最后落在某一个坑的底部，它没有办法再往下滚动了，它就会停在这里，我们训练神经网络的过程也差不多是这样子吧

通过寻找一个线性函数来，你和x和y之间的关系的方法吗？在机器学习中，我们通常叫做线性回归。

神经网络是通过线性函数和非线性激活函数不断复合组合得到的复杂非线性函数那么对应的它的损失函数也会是一个非线性函数。那往往不能根据刚刚说的导数令导数等于零的方式来求解这个神经网络，这里我们就引入梯度下降、反向传播算法。

具体来讲是这样的，我们先随机初始化一个W和B的值，然后一点一点调整W和B，让损失函数产生一点变化，如果说我们这次调整W让损失函数减小了，那说明这个变化是有效的，如果说反过来我们把b的值往大了调，导致损失函数变大了，那说明这次调整是不利于损失函数减小的，我们就要反过来把b往小了调整，要不断尝试，不断调整，让损失函数往下降的趋势上不断走。那么我们在代码里面怎么知道这一次对W调整它对于损失函数是增大还是减小呢？我们其实只要求解损失函数对W的偏导，就知道这一个W和损失函数的变化关系是怎么样的了。偏导数为正说明W增加会导致损失函数增加，如果偏导数为负，那么说明W的增加会导致损失函数的减小，那么我们要让损失函数变小的话呢，我们就拿调整前的W减去当前损失函数对当前W的偏导数来实现让新的W能够减小损失函数，我们假设这个调整的步长等于偏导数，这样子就能够保证说我们调整一个参数得到新的值之后，这个损失函数是降低的。

有时候直接减去这个偏导数会导致调整的幅度过大，可能会引起梯度爆炸或者梯度消失的问题，因此我们还会给这个偏导数乘以一个参数来控制我们这一次迭代网络时调整参数的幅度的大小，这个参数我们通常叫做学习率，这种网络之外的引入的调整的参数我们通常叫为超参

然后我们网络的参数有很多，这个损失函数对网络的每一个参数的偏导数，我们通常称之为梯度。

然后我们要怎么样去求解损失函数？偏导数呢？由于损失函数，由于神经网络的计划函数本身就是一个非常复杂的推荐性函数，那损失函数肯定也是一个更复杂的推荐性函数，直接对于这个损失函数求导显然是不可能的。考虑一个最简单的神经网络结构，一个输出，一个输出神经元，一个输入神经元，一层隐藏层的形状，数据的计算过程就是从X到A到Yhat再到L损失函数，然后如何求损失函数L对W的偏导？
实际上L对W的偏导就是W变化了一点点会导致L变化多少，由于神经网络的传播效应，前向传播W变化一点会导致A变化一点点，A变化一点点又会导致Yhat变化一点点，然后Yhat变化一点点会导致L变化一点点，按照这个传播路径来求导，我们应该先求L对于Yhat的偏导，再求Yhat对于A的偏导，再求A对于W一的偏导，再把它们乘起来，最后就得到了L对于W的偏导这个我们叫做偏导的链式法则，但其实就是复合函数求导。