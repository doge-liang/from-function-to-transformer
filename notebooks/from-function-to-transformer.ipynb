{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从函数到 Transformer：一步步理解现代深度学习架构\n",
    "\n",
    "本笔记将带领你从最基础的函数概念出发，逐步理解现代深度学习架构的核心原理。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 两种思维范式：符号主义 vs 连接主义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 符号主义 (Symbolic AI)\n",
    "\n",
    "**核心思想**：通过人类的观察和总结，发现事物背后的规律，用明确的数学函数来描述。\n",
    "\n",
    "**例子**：勾股定理 $a^2 + b^2 = c^2$\n",
    "\n",
    "**局限性**：当问题足够复杂时，人类无法总结出明确的规律（如翻译、天气预测）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 连接主义 (Connectionism)\n",
    "\n",
    "**核心思想**：不试图发现明确规律，而是根据输入-输出数据来\"猜\"出规律。\n",
    "\n",
    "**步骤**：\n",
    "1. 拿到输入数据 X 和输出数据 Y\n",
    "2. 假设一个函数形式（参数待定）\n",
    "3. 计算预测值与真实值的偏差\n",
    "4. 最小化偏差，得到近似规律"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**最小二乘法示例**：一元一次线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 示例数据：y = 2x + 1 + 噪声\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 20)\n",
    "y_true = 2 * X + 1\n",
    "y = y_true + np.random.randn(20) * 2  # 添加噪声\n",
    "\n",
    "# 使用最小二乘法拟合\n",
    "# y = kx + b => 求解 k 和 b\n",
    "n = len(X)\n",
    "k = (n * np.sum(X * y) - np.sum(X) * np.sum(y)) / (n * np.sum(X**2) - np.sum(X)**2)\n",
    "b = (np.sum(y) - k * np.sum(X)) / n\n",
    "\n",
    "print(f\"真实关系: y = 2x + 1\")\n",
    "print(f\"拟合结果: y = {k:.4f}x + {b:.4f}\")\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X, y, alpha=0.7, label='带噪声的数据')\n",
    "plt.plot(X, k * X + b, 'r-', linewidth=2, label='拟合的直线')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('最小二乘法线性回归')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 从线性到非线性：激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 为什么需要非线性？\n",
    "\n",
    "现实世界中的规律往往是**非线性**的。线性函数（直线）无法拟合复杂的现实规律。\n",
    "\n",
    "**解决方案**：引入非线性函数（激活函数）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 常用激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "# Sigmoid: σ(x) = 1 / (1 + e^(-x))\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "\n",
    "# ReLU: max(0, x)\n",
    "relu = np.maximum(0, x)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(x, sigmoid, 'b-', linewidth=2)\n",
    "axes[0].set_title('Sigmoid 激活函数', fontsize=12)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('σ(x)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0].axhline(y=1, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "axes[1].plot(x, relu, 'g-', linewidth=2)\n",
    "axes[1].set_title('ReLU 激活函数', fontsize=12)\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('ReLU(x)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Sigmoid: 输出范围 (0, 1)，适合二分类\")\n",
    "print(\"ReLU: 输出范围 [0, +∞)，是现在最常用的激活函数\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 神经网络的结构\n",
    "\n",
    "- **线性部分**：$z = W \\cdot x + b$\n",
    "- **非线性部分**：$a = \\text{激活函数}(z)$\n",
    "\n",
    "多层叠加：神经网络 = 多个线性变换 + 激活函数 组合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 神经网络结构详解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 网络结构示意图\n",
    "\n",
    "```\n",
    "输入层          隐藏层          输出层\n",
    "  ○              ○              ○\n",
    "  │         ╱    │    ╲         │\n",
    "  │        ╱     │     ╲        │\n",
    "  ○       ○      ○      ○       ○\n",
    "  │      ╱│╲    ╱│╲    ╱│╲      │\n",
    "  ○     ○○○    ○○○    ○○○     ○\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 前向传播 (Forward Propagation)\n",
    "\n",
    "数据从输入层 → 隐藏层 → 输出层的传播过程。\n",
    "\n",
    "**公式**：\n",
    "$z^{(l)} = W^{(l)} \\cdot a^{(l-1)} + b^{(l)}$\n",
    "\n",
    "$a^{(l)} = \\text{激活函数}(z^{(l)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动实现一个简单的前向传播\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# 简单的 2 层神经网络 (输入 -> 隐藏层 -> 输出)\n",
    "# 输入层: 2 个神经元\n",
    "# 隐藏层: 3 个神经元\n",
    "# 输出层: 1 个神经元\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 初始化权重和偏置\n",
    "W1 = np.random.randn(2, 3)   # 输入到隐藏层: 2x3\n",
    "b1 = np.random.randn(3)      # 隐藏层偏置: 3个\n",
    "W2 = np.random.randn(3, 1)   # 隐藏层到输出层: 3x1\n",
    "b2 = np.random.randn(1)      # 输出层偏置: 1个\n",
    "\n",
    "# 前向传播\n",
    "def forward(X):\n",
    "    # 输入 -> 隐藏层\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    # 隐藏层 -> 输出层\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    return a2, (z1, a1, z2)\n",
    "\n",
    "# 测试\n",
    "X_test = np.array([[0.5, 1.0]])\n",
    "output, cache = forward(X_test)\n",
    "print(f\"输入: {X_test}\")\n",
    "print(f\"输出: {output[0][0]:.4f}\")\n",
    "print(f\"\\n中间结果:\")\n",
    "print(f\"  隐藏层输入 z1: {cache[0]}\")\n",
    "print(f\"  隐藏层输出 a1: {cache[1]}\")\n",
    "print(f\"  输出层输入 z2: {cache[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
  .parameters": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 损失函数：衡量预测的好坏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 均方误差 (MSE)\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**为什么用平方而不是绝对值？**\n",
    "- 平方可导，便于计算\n",
    "- 平方让大误差受到更大惩罚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"计算均方误差\"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# 示例\n",
    "y_true = np.array([1, 2, 3, 4, 5])\n",
    "y_pred = np.array([1.1, 1.9, 3.2, 3.8, 4.9])\n",
    "\n",
    "loss = mse_loss(y_true, y_pred)\n",
    "print(f\"均方误差 (MSE): {loss:.4f}\")\n",
    "\n",
    "# 可视化误差\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, 6), y_true, 'bo-', label='真实值', markersize=10)\n",
    "plt.plot(range(1, 6), y_pred, 'rx--', label='预测值', markersize=10)\n",
    "for i in range(5):\n",
    "    plt.vlines(i+1, min(y_true[i], y_pred[i]), max(y_true[i], y_pred[i]), \n",
    "               colors='red', alpha=0.5, linestyles='dotted')\n",
    "plt.xlabel('样本')\n",
    "plt.ylabel('值')\n",
    "plt.title(f'预测误差可视化 (MSE = {loss:.4f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 梯度下降：找到最优参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 核心思想\n",
    "\n",
    "想象一个乒乓球在凹凸不平的地面上滚动：\n",
    "- 重力让它往低处滚\n",
    "- 最终停在某个\"坑底\"（极小值点）\n",
    "\n",
    "**梯度下降公式**：\n",
    "\n",
    "$$W_{new} = W_{old} - \\eta \\cdot \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "其中 $\\eta$ 是学习率 (learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化梯度下降过程\n",
    "def f(x):\n",
    "    \"\"\"一个简单的凸函数\"\"\"\n",
    "    return x**2\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"导数\"\"\"\n",
    "    return 2 * x\n",
    "\n",
    "# 梯度下降\n",
    "x = 4.5  # 起点\n",
    "lr = 0.1  # 学习率\n",
    "history = [x]\n",
    "\n",
    "for i in range(20):\n",
    "    grad = df(x)\n",
    "    x = x - lr * grad\n",
    "    history.append(x)\n",
    "\n",
    "# 可视化\n",
    "x_range = np.linspace(-5, 5, 100)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x_range, f(x_range), 'b-', linewidth=2, label='f(x) = x²')\n",
    "plt.plot(history, [f(x) for x in history], 'ro-', markersize=8, label='梯度下降路径')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title(f'梯度下降过程 (从 x={history[0]:.1f} 到 x={history[-1]:.4f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"起点: x = {history[0]}\")\n",
    "print(f\"终点: x = {history[-1]:.4f}\")\n",
    "print(f\"最小值点理论值: x = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 学习率的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "learning_rates = [0.01, 0.3, 0.9]\n",
    "titles = ['学习率太小 (收敛慢)', '学习率适中 (理想)', '学习率太大 (震荡)']\n",
    "\n",
    "for ax, lr, title in zip(axes, learning_rates, titles):\n",
    "    x = 4.5\n",
    "    history = [x]\n",
    "    \n",
    "    for _ in range(15):\n",
    "        grad = df(x)\n",
    "        x = x - lr * grad\n",
    "        history.append(x)\n",
    "    \n",
    "    ax.plot(x_range, f(x_range), 'b-', linewidth=1)\n",
    "    ax.plot(history, [f(x) for x in history], 'ro-', markersize=6)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('f(x)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(-5, 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"学习率太小时收敛慢，学习率太大时可能无法收敛\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 反向传播：链式法则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 链式法则 (Chain Rule)\n",
    "\n",
    "神经网络是一个复合函数：\n",
    "\n",
    "$$L = f(g(h(x)))$$\n",
    "\n",
    "**链式法则**：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial f} \\cdot \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial h} \\cdot \\frac{\\partial h}{\\partial x}$$\n",
    "\n",
    "**在神经网络中**：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \n",
    "\\frac{\\partial L}{\\partial \\hat{y}} \\cdot \n",
    "\\frac{\\partial \\hat{y}}{\\partial a} \\cdot \n",
    "\\frac{\\partial a}{\\partial z} \\cdot \n",
    "\\frac{\\partial z}{\\partial W}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 PyTorch 实现一个完整的神经网络训练过程\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 生成训练数据: y = 2x + 1 + 噪声\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "X_train = torch.randn(100, 1) * 5  # 100个样本\n",
    "y_train = 2 * X_train + 1 + torch.randn(100, 1)  # 添加噪声\n",
    "\n",
    "# 定义神经网络\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(1, 10)  # 隐藏层: 10个神经元\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(10, 1)  # 输出层\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 训练\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "    # 前向传播\n",
    "    predictions = model(X_train)\n",
    "    loss = criterion(predictions, y_train)\n",
    "    \n",
    "    # 反向传播\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# 可视化训练过程\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('训练损失曲线')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_plot = torch.linspace(-10, 10, 100).reshape(-1, 1)\n",
    "    y_plot = model(X_plot)\n",
    "    \n",
    "axes[1].scatter(X_train, y_train, alpha=0.5, label='训练数据')\n",
    "axes[1].plot(X_plot, y_plot, 'r-', linewidth=2, label='神经网络拟合')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('神经网络拟合结果')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 反向传播的工作原理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化反向传播的梯度流动\n",
    "# 假设一个简单的计算图\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2      # y = x²\n",
    "z = y * 3       # z = 3y = 3x²\n",
    "w = z + 1       # w = z + 1 = 3x² + 1\n",
    "\n",
    "# 反向传播\n",
    "w.backward()\n",
    "\n",
    "print(\"计算图: x -> y = x² -> z = 3y -> w = z + 1\")\n",
    "print(f\"\\n前向传播:\")\n",
    "print(f\"  x = {x.item()}\")\n",
    "print(f\"  y = x² = {y.item()}\")\n",
    "print(f\"  z = 3y = {z.item()}\")\n",
    "print(f\"  w = z + 1 = {w.item()}\")\n",
    "print(f\"\\n反向传播 (梯度):\")\n",
    "print(f\"  dw/dx = {x.grad.item():.1f} (链式法则: ∂w/∂x = ∂w/∂z * ∂z/∂y * ∂y/∂x = 1 * 3 * 2x = 6x)\")\n",
    "\n",
    "# 验证: ∂w/∂x = 6x = 6*2 = 12\n",
    "print(f\"\\n验证: 6 * x = 6 * {x.item()} = {6 * x.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 总结：神经网络的训练流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                    神经网络训练流程                      │\n",
    "├─────────────────────────────────────────────────────────┤\n    "│                                                         │\n",
    "│  1. 初始化: 随机初始化权重 W 和偏置 b                   │\n",
    "│              ↓                                          │\n",
    "│  2. 前向传播: 输入 → 隐藏层 → 输出层                    │\n",
    "│              ↓                                          │\n",
    "│  3. 计算损失: MSE = Σ(y - ŷ)² / n                      │\n",
    "│              ↓                                          │\n",
    "│  4. 反向传播: 计算 ∂L/∂W (链式法则)                    │\n",
    "│              ↓                                          │\n",
    "│  5. 梯度下降: W = W - η * ∂L/∂W                        │\n",
    "│              ↓                                          │\n",
    "│  6. 重复步骤 2-5 直到损失足够小                         │\n",
    "│                                                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 下一步：探索 Transformer\n",
    "\n",
    "在理解了这些基础概念后，你可以继续探索：\n",
    "\n",
    "- **注意力机制 (Attention)**：让模型学会关注最重要的输入\n",
    "- **自注意力 (Self-Attention)**：序列内部的关系建模\n",
    "- **位置编码 (Positional Encoding)**：为序列添加位置信息\n",
    "- **多头注意力 (Multi-Head Attention)**：并行学习多种关系\n",
    "- **Encoder-Decoder 架构**：处理序列到序列的任务"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
