{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 从函数到 Transformer：一步步理解现代深度学习架构",
        "",
        "本笔记将带领你从最基础的函数概念出发，逐步理解现代深度学习架构的核心原理。",
        "",
        "---",
        "",
        "## 1. 两种思维范式：符号主义 vs 连接主义"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 符号主义 (Symbolic AI)",
        "",
        "**核心思想**：通过人类的观察和总结，发现事物背后的规律，用明确的数学函数来描述。",
        "",
        "**例子**：勾股定理 a^2 + b^2 = c^2",
        "",
        "**局限性**：当问题足够复杂时，人类无法总结出明确的规律（如翻译、天气预测）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 连接主义 (Connectionism)",
        "",
        "**核心思想**：不试图发现明确规律，而是根据输入-输出数据来\"猜\"出规律。",
        "",
        "**步骤**：",
        "1. 拿到输入数据 X 和输出数据 Y",
        "2. 假设一个函数形式（参数待定）",
        "3. 计算预测值与真实值的偏差",
        "4. 最小化偏差，得到近似规律"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**最小二乘法示例**：一元一次线性回归"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# 示例数据：y = 2x + 1 + 噪声\nnp.random.seed(42)\nX = np.linspace(0, 10, 20)\ny_true = 2 * X + 1\ny = y_true + np.random.randn(20) * 2  # 添加噪声\n\n# 使用最小二乘法拟合\nn = len(X)\nk = (n * np.sum(X * y) - np.sum(X) * np.sum(y)) / (n * np.sum(X**2) - np.sum(X)**2)\nb = (np.sum(y) - k * np.sum(X)) / n\n\nprint(f\"真实关系: y = 2x + 1\")\nprint(f\"拟合结果: y = {k:.4f}x + {b:.4f}\")\n\n# 可视化\nplt.figure(figsize=(8, 5))\nplt.scatter(X, y, alpha=0.7, label='带噪声的数据')\nplt.plot(X, k * X + b, 'r-', linewidth=2, label='拟合的直线')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('最小二乘法线性回归')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "## 2. 从线性到非线性：激活函数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 为什么需要非线性？",
        "",
        "现实世界中的规律往往是**非线性**的。线性函数（直线）无法拟合复杂的现实规律。",
        "",
        "**解决方案**：引入非线性函数（激活函数）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 常用激活函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-5, 5, 100)\n\n# Sigmoid: sigma(x) = 1 / (1 + e^(-x))\nsigmoid = 1 / (1 + np.exp(-x))\n\n# ReLU: max(0, x)\nrelu = np.maximum(0, x)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\naxes[0].plot(x, sigmoid, 'b-', linewidth=2)\naxes[0].set_title('Sigmoid 激活函数', fontsize=12)\naxes[0].set_xlabel('x')\naxes[0].set_ylabel('sigma(x)')\naxes[0].grid(True, alpha=0.3)\naxes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[0].axhline(y=1, color='k', linestyle='--', alpha=0.3)\n\naxes[1].plot(x, relu, 'g-', linewidth=2)\naxes[1].set_title('ReLU 激活函数', fontsize=12)\naxes[1].set_xlabel('x')\naxes[1].set_ylabel('ReLU(x)')\naxes[1].grid(True, alpha=0.3)\naxes[1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Sigmoid: 输出范围 (0, 1)，适合二分类\")\nprint(\"ReLU: 输出范围 [0, +inf)，是现在最常用的激活函数\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 神经网络的结构",
        "",
        "- **线性部分**：z = W * x + b",
        "- **非线性部分**：a = 激活函数(z)",
        "",
        "多层叠加：神经网络 = 多个线性变换 + 激活函数 组合"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "## 3. 神经网络结构详解"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 网络结构示意图",
        "",
        "```",
        "输入层          隐藏层          输出层",
        "  O              O              O",
        "  |         /    |    \\         |",
        "  |        /     |     \\        |",
        "  O       O      O      O       O",
        "  |      /|\\    /|\\    /|\\      |",
        "  O     OOO    OOO    OOO     O",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 前向传播 (Forward Propagation)",
        "",
        "数据从输入层 -> 隐藏层 -> 输出层的传播过程。",
        "",
        "**公式**：",
        "z^(l) = W^(l) * a^(l-1) + b^(l)",
        "",
        "a^(l) = 激活函数(z^(l))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 手动实现一个简单的前向传播\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# 简单的 2 层神经网络\nnp.random.seed(42)\n\n# 初始化权重和偏置\nW1 = np.random.randn(2, 3)   # 输入到隐藏层: 2x3\nb1 = np.random.randn(3)      # 隐藏层偏置: 3个\nW2 = np.random.randn(3, 1)   # 隐藏层到输出层: 3x1\nb2 = np.random.randn(1)      # 输出层偏置: 1个\n\n# 前向传播\ndef forward(X):\n    z1 = np.dot(X, W1) + b1\n    a1 = sigmoid(z1)\n    z2 = np.dot(a1, W2) + b2\n    a2 = sigmoid(z2)\n    return a2, (z1, a1, z2)\n\n# 测试\nX_test = np.array([[0.5, 1.0]])\noutput, cache = forward(X_test)\nprint(f\"输入: {X_test}\")\nprint(f\"输出: {output[0][0]:.4f}\")\nprint(f\"\\n中间结果:\")\nprint(f\"  隐藏层输入 z1: {cache[0]}\")\nprint(f\"  隐藏层输出 a1: {cache[1]}\")\nprint(f\"  输出层输入 z2: {cache[2]}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "## 4. 损失函数：衡量预测的好坏"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 均方误差 (MSE)",
        "",
        "MSE = (1/n) * sum((y_i - y_hat_i)^2)",
        "",
        "**为什么用平方而不是绝对值？**",
        "- 平方可导，便于计算",
        "- 平方让大误差受到更大惩罚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def mse_loss(y_true, y_pred):\n    return np.mean((y_true - y_pred) ** 2)\n\n# 示例\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1.1, 1.9, 3.2, 3.8, 4.9])\n\nloss = mse_loss(y_true, y_pred)\nprint(f\"均方误差 (MSE): {loss:.4f}\")\n\n# 可视化误差\nplt.figure(figsize=(8, 4))\nplt.plot(range(1, 6), y_true, 'bo-', label='真实值', markersize=10)\nplt.plot(range(1, 6), y_pred, 'rx--', label='预测值', markersize=10)\nfor i in range(5):\n    plt.vlines(i+1, min(y_true[i], y_pred[i]), max(y_true[i], y_pred[i]), \n               colors='red', alpha=0.5, linestyles='dotted')\nplt.xlabel('样本')\nplt.ylabel('值')\nplt.title(f'预测误差可视化 (MSE = {loss:.4f})')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "## 5. 梯度下降：找到最优参数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 核心思想",
        "",
        "想象一个乒乓球在凹凸不平的地面上滚动：",
        "- 重力让它往低处滚",
        "- 最终停在某个\"坑底\"（极小值点）",
        "",
        "**梯度下降公式**：",
        "",
        "W_new = W_old - learning_rate * (dL/dW)",
        "",
        "其中 learning_rate 是学习率"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 可视化梯度下降过程\ndef f(x):\n    return x**2\n\ndef df(x):\n    return 2 * x\n\n# 梯度下降\nx = 4.5  # 起点\nlr = 0.1  # 学习率\nhistory = [x]\n\nfor i in range(20):\n    grad = df(x)\n    x = x - lr * grad\n    history.append(x)\n\n# 可视化\nx_range = np.linspace(-5, 5, 100)\nplt.figure(figsize=(10, 5))\nplt.plot(x_range, f(x_range), 'b-', linewidth=2, label='f(x) = x^2')\nplt.plot(history, [f(x) for x in history], 'ro-', markersize=8, label='梯度下降路径')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title(f'梯度下降过程 (从 x={history[0]:.1f} 到 x={history[-1]:.4f})')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"起点: x = {history[0]}\")\nprint(f\"终点: x = {history[-1]:.4f}\")\nprint(f\"最小值点理论值: x = 0\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 学习率的影响"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\nlearning_rates = [0.01, 0.3, 0.9]\ntitles = ['学习率太小 (收敛慢)', '学习率适中 (理想)', '学习率太大 (震荡)']\n\nfor ax, lr, title in zip(axes, learning_rates, titles):\n    x = 4.5\n    history = [x]\n    \n    for _ in range(15):\n        grad = df(x)\n        x = x - lr * grad\n        history.append(x)\n    \n    ax.plot(x_range, f(x_range), 'b-', linewidth=1)\n    ax.plot(history, [f(x) for x in history], 'ro-', markersize=6)\n    ax.set_title(title)\n    ax.set_xlabel('x')\n    ax.set_ylabel('f(x)')\n    ax.grid(True, alpha=0.3)\n    ax.set_xlim(-5, 5)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"学习率太小时收敛慢，学习率太大时可能无法收敛\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "## 6. 反向传播：链式法则"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 链式法则 (Chain Rule)",
        "",
        "神经网络是一个复合函数：",
        "",
        "L = f(g(h(x)))",
        "",
        "**链式法则**：",
        "",
        "dL/dx = (dL/df) * (df/dg) * (dg/dh) * (dh/dx)",
        "",
        "**在神经网络中**：",
        "",
        "dL/dW = (dL/dy_hat) * (dy_hat/da) * (da/dz) * (dz/dW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 使用 PyTorch 实现一个完整的神经网络训练过程\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# 生成训练数据: y = 2x + 1 + 噪声\nnp.random.seed(42)\ntorch.manual_seed(42)\n\nX_train = torch.randn(100, 1) * 5  # 100个样本\ny_train = 2 * X_train + 1 + torch.randn(100, 1)  # 添加噪声\n\n# 定义神经网络\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.hidden = nn.Linear(1, 10)  # 隐藏层: 10个神经元\n        self.relu = nn.ReLU()\n        self.output = nn.Linear(10, 1)  # 输出层\n    \n    def forward(self, x):\n        x = self.relu(self.hidden(x))\n        x = self.output(x)\n        return x\n\nmodel = SimpleNN()\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# 训练\nlosses = []\nfor epoch in range(100):\n    predictions = model(X_train)\n    loss = criterion(predictions, y_train)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    losses.append(loss.item())\n    \n    if (epoch + 1) % 20 == 0:\n        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n\n# 可视化训练过程\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\naxes[0].plot(losses)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('训练损失曲线')\naxes[0].grid(True, alpha=0.3)\n\nwith torch.no_grad():\n    X_plot = torch.linspace(-10, 10, 100).reshape(-1, 1)\n    y_plot = model(X_plot)\n    \naxes[1].scatter(X_train, y_train, alpha=0.5, label='训练数据')\naxes[1].plot(X_plot, y_plot, 'r-', linewidth=2, label='神经网络拟合')\naxes[1].set_xlabel('x')\naxes[1].set_ylabel('y')\naxes[1].set_title('神经网络拟合结果')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 反向传播的工作原理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 可视化反向传播的梯度流动\nimport torch\n\nx = torch.tensor(2.0, requires_grad=True)\ny = x ** 2      # y = x^2\nz = y * 3       # z = 3y = 3x^2\nw = z + 1       # w = z + 1 = 3x^2 + 1\n\n# 反向传播\nw.backward()\n\nprint(\"计算图: x -> y = x^2 -> z = 3y -> w = z + 1\")\nprint(f\"\\n前向传播:\")\nprint(f\"  x = {x.item()}\")\nprint(f\"  y = x^2 = {y.item()}\")\nprint(f\"  z = 3y = {z.item()}\")\nprint(f\"  w = z + 1 = {w.item()}\")\nprint(f\"\\n反向传播 (梯度):\")\nprint(f\"  dw/dx = {x.grad.item():.1f} (链式法则)\")\n\nprint(f\"\\n验证: 6 * x = 6 * {x.item()} = {6 * x.item()}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "## 7. 总结：神经网络的训练流程"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```",
        "+---------------------------------------------------------+",
        "|           神经网络训练流程                               |",
        "+---------------------------------------------------------+",
        "|                                                         |",
        "|  1. 初始化: 随机初始化权重 W 和偏置 b                   |",
        "|              |                                          |",
        "|  2. 前向传播: 输入 -> 隐藏层 -> 输出层                  |",
        "|              |                                          |",
        "|  3. 计算损失: MSE = sum((y - y_hat)^2) / n             |",
        "|              |                                          |",
        "|  4. 反向传播: 计算 dL/dW (链式法则)                    |",
        "|              |                                          |",
        "|  5. 梯度下降: W = W - lr * dL/dW                       |",
        "|              |                                          |",
        "|  6. 重复步骤 2-5 直到损失足够小                         |",
        "|                                                         |",
        "+---------------------------------------------------------+",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "## 8. 下一步：探索 Transformer",
        "",
        "在理解了这些基础概念后，你可以继续探索：",
        "",
        "- **注意力机制 (Attention)**：让模型学会关注最重要的输入",
        "- **自注意力 (Self-Attention)**：序列内部的关系建模",
        "- **位置编码 (Positional Encoding)**：为序列添加位置信息",
        "- **多头注意力 (Multi-Head Attention)**：并行学习多种关系",
        "- **Encoder-Decoder 架构**：处理序列到序列的任务"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}